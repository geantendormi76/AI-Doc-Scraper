# -*- coding: utf-8 -*-
import asyncio
import aiohttp
import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md
import logging
import os
import re
from urllib.parse import urljoin, urlparse
from time import time
from tqdm.asyncio import tqdm

# --- å…¨å±€é…ç½® ---
START_URL = "https://docs.isaacsim.omniverse.nvidia.com/4.5.0/index.html"
BASE_URL = "https://docs.isaacsim.omniverse.nvidia.com/4.5.0/"
OUTPUT_DIR = "isaac_sim_4.5_docs_md"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
CONCURRENT_REQUESTS = 20

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

def get_all_doc_urls(start_url, base_url):
    """ä»èµ·å§‹é¡µé¢è·å–æ‰€æœ‰ç‹¬ç«‹çš„æ–‡æ¡£é¡µé¢URLã€‚"""
    logging.info(f"æ­¥éª¤ 1: å¼€å§‹ä» {start_url} è·å–æ‰€æœ‰URLé“¾æ¥...")
    try:
        response = requests.get(start_url, headers=HEADERS, timeout=20)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        logging.error(f"æ— æ³•è·å–å…¥å£é¡µé¢ {start_url}: {e}")
        return []

    soup = BeautifulSoup(response.text, 'html.parser')
    urls = set()
    nav_container = soup.find('div', class_='bd-sidebar-primary')
    if not nav_container:
        logging.error("æœªæ‰¾åˆ°å¯¼èˆªå®¹å™¨ (class='bd-sidebar-primary')ã€‚è¯·æ£€æŸ¥ç½‘é¡µç»“æ„ã€‚")
        return []

    for link in nav_container.find_all('a', href=True):
        href = link['href']
        if href.startswith('#') or 'javascript:void(0)' in href:
            continue
        
        absolute_url = urljoin(base_url, href)
        page_url = absolute_url.split('#')[0].split('?')[0]
        
        if urlparse(page_url).netloc == urlparse(base_url).netloc and page_url.endswith('.html'):
            urls.add(page_url)
        
    logging.info(f"âœ… æ­¥éª¤ 1 å®Œæˆ: æˆåŠŸæ‰¾åˆ° {len(urls)} ä¸ªç‹¬ç«‹çš„URLã€‚")
    return sorted(list(urls))

def generate_safe_filename(url):
    """ä»URLç”Ÿæˆä¸€ä¸ªå®‰å…¨çš„æ–‡ä»¶åï¼Œå°†'/'å’Œ'-'éƒ½æ›¿æ¢ä¸º'_'ã€‚"""
    path = urlparse(url).path
    path = path.replace('/4.5.0/', '')
    safe_name = path.replace('/', '_').replace('-', '_').replace('.html', '').strip('_')
    safe_name = re.sub(r'[^a-zA-Z0-9_]', '', safe_name)
    return f"{safe_name}.md"

def clean_and_convert_to_markdown(html_content):
    """æ¸…ç†HTMLå¹¶å°†å…¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚"""
    soup = BeautifulSoup(html_content, 'html.parser')
    main_content = soup.find('main', id='main-content')
    if not main_content:
        logging.warning("åœ¨HTMLä¸­æœªæ‰¾åˆ°ä¸»å†…å®¹åŒºåŸŸ (id='main-content')ã€‚")
        return ""

    elements_to_remove_selectors = [
        'div.edit-this-page',
        'a.headerlink',
        'div.prev-next-area'
    ]
    for selector in elements_to_remove_selectors:
        for element in main_content.select(selector):
            element.decompose()

    return md(str(main_content), heading_style="ATX")

async def fetch_and_process(session, url, output_dir, semaphore):
    """å¼‚æ­¥æŠ“å–ã€å¤„ç†å¹¶ä¿å­˜Markdownæ–‡ä»¶ï¼ŒåŒæ—¶åœ¨æ–‡ä»¶é¡¶éƒ¨åµŒå…¥åŸå§‹URLã€‚"""
    async with semaphore:
        try:
            async with session.get(url) as response:
                if response.status != 200:
                    logging.warning(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç  {response.status} for {url}")
                    return
                
                html = await response.text()
                markdown_content = clean_and_convert_to_markdown(html)
                
                if not markdown_content:
                    logging.warning(f"åœ¨ {url} æœªæ‰¾åˆ°æˆ–æ— æ³•æå–ä¸»å†…å®¹ã€‚")
                    return

                # --- æ ¸å¿ƒä¿®æ­£ï¼šåœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ URLå…ƒæ•°æ® ---
                final_content = f"<!-- Original URL: {url} -->\n\n{markdown_content}"

                filename = generate_safe_filename(url)
                filepath = os.path.join(output_dir, filename)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(final_content)
                
        except Exception as e:
            logging.error(f"å¤„ç†URLæ—¶å‘ç”Ÿé”™è¯¯ {url}: {e}")

async def main():
    start_time = time()
    
    doc_urls = get_all_doc_urls(START_URL, BASE_URL)
    if not doc_urls:
        return

    if os.path.exists(OUTPUT_DIR):
        import shutil
        shutil.rmtree(OUTPUT_DIR)
        logging.info(f"å·²æ¸…ç†æ—§çš„è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    os.makedirs(OUTPUT_DIR)
    logging.info(f"å·²åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")

    semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS)
    async with aiohttp.ClientSession(headers=HEADERS) as session:
        total_urls = len(doc_urls)
        logging.info(f"\næ­¥éª¤ 2: å¼€å§‹ä» {total_urls} ä¸ªURLä¸­å¼‚æ­¥æŠ“å–å¹¶è½¬æ¢ä¸ºMarkdown...")
        tasks = [fetch_and_process(session, url, OUTPUT_DIR, semaphore) for url in doc_urls]
        await tqdm.gather(*tasks, desc="æ­£åœ¨æŠ“å–æ–‡æ¡£")

    end_time = time()
    logging.info("\nğŸ‰ å…¨éƒ¨å¤„ç†å®Œæ¯•ï¼ ğŸ‰")
    logging.info(f"æ‰€æœ‰Markdownæ–‡ä»¶å·²ä¿å­˜åœ¨ '{OUTPUT_DIR}' æ–‡ä»¶å¤¹ä¸­ã€‚")
    logging.info(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’ã€‚")

if __name__ == "__main__":
    asyncio.run(main())